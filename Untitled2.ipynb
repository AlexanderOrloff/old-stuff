{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymorphy2\n",
    "import string\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(data):                          #препроцессинг (убираем знаки препинания и приводим всё к начальной форме)\n",
    "            data = data.split()\n",
    "            text = ''\n",
    "            for word in data:\n",
    "                word = word.strip('[!,.?\"]')\n",
    "                p = morph.parse(word.strip())[0]\n",
    "                p = p.normal_form\n",
    "                if p != '-':\n",
    "                    if p != '00'                       #!\n",
    "                    text  = text + ' ' + p\n",
    "            return text[1:]\n",
    "\n",
    "\n",
    "\n",
    "def get_texts():                                     # находим тексты для индексирования                              \n",
    "    texts = {}\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    for root, dirs, files in os.walk('./friends'):\n",
    "        for dir in dirs:\n",
    "            for file in os.listdir(os.path.join('./friends', dir)):\n",
    "                with open(os.path.join('.', 'friends', dir, file), 'r', encoding = 'utf-8') as f:\n",
    "                        f = f.read()\n",
    "                        text = preproc(f)\n",
    "                        texts[os.path.join('.', 'friends', dir, file)] = text\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    return [preproc(input())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-work\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(get_texts())\n",
    "voc = vectorizer.get_feature_names()\n",
    "#filenames = list_of_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17', '1x01', '1x02', '1x03', '1x04', '1x05', '1x06', '1x07', '1x08', '1x09', '1x10', '1x11', '1x12', '1x13', '1x14', '1x15', '1x16', '1x17', '1x18', '1x19', '1x20', '24', '25', '26', '2x01', '2x02', '2x03', '2x04', '2x05', '2x06', '2x07', '2x08', '2x09', '2x10', '2x11', '2x12', '2x13', '2x14', '2x15', '2x16', '2x17', '2x18', '2x19', '2x20', '2x21', '2x22', '2x23', '2x24', '3x01', '3x02', '3x03', '3x04', '3x05', '3x06', '3x07', '3x08', '3x09', '3x10', '3x11', '3x12', '3x13', '3x14', '3x15', '3x16', '3x17', '3x18', '3x19', '3x20', '3x21', '4x01', '4x02', '4x03', '4x04', '4x05', '4x06', '4x07', '4x08', '4x09', '4x10', '4x11', '4x12', '4x13', '4x14', '4x15', '4x16', '4x17', '4x18', '4x19', '4x20', '4x21', '4x22', '4x23', '4x24', '5x01', '5x02', '5x03', '5x04', '5x05', '5x06', '5x07', '5x08', '5x09', '5x10', '5x11', '5x12', '5x13', '5x14', '5x15', '5x16', '5x17', '5x18', '5x19', '5x20', '5x21', '5x22', '5x23', '5x24', '6x01', '6x02', '6x03', '6x04', '6x05', '6x06', '6x07', '6x08', '6x09', '6x10', '6x11', '6x12', '6x13', '6x14', '6x15', '6x16', '6x17', '6x18', '6x19', '6x20', '6x21', '6x22', '6x23', '6x24', '6x25', '7x01', '7x02', '7x03', '7x04', '7x05', '7x06', '7x07', '7x08', '7x09', '7x10', '7x11', '7x12', '7x13', '7x14', '7x15', '7x16', '7x17', '7x18', '7x19', '7x20', '7x21', '7x22', '7x23', '7x24', 'about', 'after', 'all', 'along', 'an', 'and', 'apothecary', 'are', 'armadillo', 'assistant', 'at', 'award', 'away', 'baby', 'bag', 'ball', 'ballroom', 'barry', 'bed', 'been', 'best', 'big', 'bing', 'blackout', 'boobies', 'book', 'bowl', 'box', 'brain', 'break', 'breast', 'bullies', 'bus', 'butt', 'can', 'candy', 'car', 'cat', 'chandler', 'cheap', 'cheesecakes', 'chick', 'chicken', 'cookies', 'cop', 'could', 'cousin', 'crosses', 'crush', 'cry', 'cuffs', 'dad', 'dancing', 'dates', 'day', 'denial', 'detergent', 'device', 'dies', 'dirty', 'doesn', 'dogs', 'dollhouse', 'dozen', 'dr', 'dress', 'dresses', 'duck', 'dvdrip', 'east', 'eddie', 'eggplant', 'elizabeth', 'embryos', 'end', 'engagement', 'ever', 'everybody', 'evil', 'ex', 'fake', 'fantasy', 'finds', 'five', 'flashback', 'flashbacks', 'flirt', 'football', 'frank', 'free', 'fridge', 'friends', 'from', 'george', 'german', 'gets', 'giant', 'girl', 'girlfriend', 'go', 'going', 'got', 'guy', 'haste', 'hates', 'have', 'hearts', 'heckles', 'high', 'his', 'hits', 'holiday', 'hugs', 'hundredth', 'husband', 'hypnosis', 'in', 'inadvertent', 'inappropriate', 'insurance', 'invitations', 'jam', 'jealousy', 'jellyfish', 'joey', 'joke', 'jr', 'just', 'kips', 'kiss', 'kissing', 'know', 'lasagnas', 'last', 'laugh', 'laundry', 'leia', 'lesbian', 'library', 'like', 'line', 'list', 'london', 'loses', 'mac', 'man', 'meets', 'metaphorical', 'milk', 'mindy', 'monica', 'monkey', 'morning', 'moves', 'mrs', 'nana', 'nap', 'new', 'night', 'no', 'nurlanb', 'old', 'on', 'one', 'orthodontist', 'out', 'parties', 'partner', 'partners', 'parts', 'party', 'paul', 'pbs', 'phoebe', 'picture', 'poker', 'poking', 'porn', 'porsche', 'poughkeepsie', 'pox', 'princess', 'prom', 'proposal', 'quits', 'race', 'rachel', 'ramoray', 're', 'ready', 'remember', 'resolutions', 'richard', 'ride', 'ring', 'roommate', 'rosita', 'ross', 'routine', 'ru', 'rugby', 'runs', 'russ', 'sandwich', 'says', 'season', 'shirt', 'sister', 'ski', 'smokes', 'sonogram', 'steaks', 'stephanopoulos', 'stoned', 'student', 'super', 'table', 'take', 'tape', 'teeth', 'thanksgiving', 'that', 'the', 'they', 'thirty', 'thumb', 'thunder', 'tiny', 'to', 'trip', 'truth', 'tunnel', 'turn', 'tv', 'twice', 'two', 'txt', 'unagi', 'underdog', 'up', 'uterus', 'vegas', 'video', 'vows', 'wedding', 'where', 'which', 'who', 'with', 'without', 'won', 'work', 'worst', 'yeller', 'yeti', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
